# osnet_pytorch_inference_optimized.py
import numpy as np
import cv2
import os
import time
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cosine
from collections import OrderedDict
import pickle
from functools import partial

# å¯¼å…¥OSNetæ¨¡å‹å®šä¹‰
import sys
sys.path.append(os.path.dirname(__file__))

from OSNet import osnet_x0_25

class OSNetPyTorchInference:
    def __init__(self, model_path, device='cuda'):
        """
        OSNet PyTorchæ¨ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬
        Args:
            model_path: osnet_x0_25.pth æ¨¡å‹è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cuda', 'cpu')
        """
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨ä¸engine.pyä¸€è‡´çš„æ–¹å¼
        start_time = time.time()
        self.model = self._load_model_engine_style(model_path)
        model_load_time = (time.time() - start_time) * 1000
        print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {model_load_time:.2f}ms")
        
        # ä½¿ç”¨ä¸engine.pyå®Œå…¨ä¸€è‡´çš„é¢„å¤„ç†å‚æ•°
        self.transform = transforms.Compose([
            transforms.Resize((256, 128)),  # ä¸engine.pyçš„crop_and_resizeä¸€è‡´
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")

    def _load_checkpoint(self, fpath):
        """ä¸engine.pyä¸€è‡´çš„checkpointåŠ è½½æ–¹å¼"""
        if fpath is None:
            raise ValueError('File path is None')
        fpath = os.path.abspath(os.path.expanduser(fpath))
        if not os.path.exists(fpath):
            raise FileNotFoundError('File is not found at "{}"'.format(fpath))
        map_location = None if torch.cuda.is_available() else 'cpu'
        try:
            checkpoint = torch.load(fpath, map_location=map_location)
        except UnicodeDecodeError:
            pickle.load = partial(pickle.load, encoding="latin1")
            pickle.Unpickler = partial(pickle.Unpickler, encoding="latin1")
            checkpoint = torch.load(
                fpath, pickle_module=pickle, map_location=map_location
            )
        except Exception:
            print('Unable to load checkpoint from "{}"'.format(fpath))
            raise
        return checkpoint

    def _load_model_engine_style(self, model_path):
        """ä½¿ç”¨ä¸engine.pyå®Œå…¨ä¸€è‡´çš„æ¨¡å‹åŠ è½½æ–¹å¼"""
        # åˆ›å»ºæ ‡å‡†OSNetæ¨¡å‹
        model = osnet_x0_25(num_classes=1, pretrained=False)
        
        if os.path.exists(model_path):
            checkpoint = self._load_checkpoint(model_path)
            
            if 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint

            model_dict = model.state_dict()
            new_state_dict = OrderedDict()
            matched_layers, discarded_layers = [], []

            for k, v in state_dict.items():
                if k.startswith('module.'):
                    k = k[7:]  # discard module.

                if k in model_dict and model_dict[k].size() == v.size():
                    new_state_dict[k] = v
                    matched_layers.append(k)
                else:
                    discarded_layers.append(k)

            model_dict.update(new_state_dict)
            model.load_state_dict(model_dict)

            if len(matched_layers) == 0:
                print(
                    'è­¦å‘Š: é¢„è®­ç»ƒæƒé‡"{}"æ— æ³•åŠ è½½ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥é”®å'.format(model_path)
                )
            else:
                print('æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡ä»"{}"'.format(model_path))
                if len(discarded_layers) > 0:
                    print('** ä»¥ä¸‹å±‚å› ä¸åŒ¹é…çš„é”®æˆ–å±‚å¤§å°è€Œè¢«ä¸¢å¼ƒ: {}'.format(discarded_layers))
        else:
            print("è­¦å‘Š: æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        
        # ç§»é™¤åˆ†ç±»å¤´ï¼Œåªä¿ç•™ç‰¹å¾æå–éƒ¨åˆ† - ä¸StrongSORTä¸€è‡´
        model.classifier = nn.Identity()
        
        return model.to(self.device)

    def preprocess_image(self, image_path):
        """
        é¢„å¤„ç†å›¾åƒ - ä¸engine.pyçš„crop_and_resizeé€»è¾‘ä¸€è‡´
        """
        # ä½¿ç”¨PILè¯»å–å›¾åƒ
        img = Image.open(image_path).convert('RGB')
        
        # åº”ç”¨è½¬æ¢
        img_tensor = self.transform(img)
        
        # æ·»åŠ batchç»´åº¦
        img_tensor = img_tensor.unsqueeze(0)
        
        return img_tensor.to(self.device)

    def preprocess_cv2_image(self, cv2_img):
        """
        å¤„ç†OpenCVå›¾åƒ - ä¸engine.pyçš„crop_and_resizeé€»è¾‘ä¸€è‡´
        """
        # è½¬æ¢BGRåˆ°RGB
        img_rgb = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(img_rgb)
        
        # åº”ç”¨è½¬æ¢
        img_tensor = self.transform(img_pil)
        img_tensor = img_tensor.unsqueeze(0)
        
        return img_tensor.to(self.device)

    def extract_features(self, image_path):
        """
        æå–ç‰¹å¾å‘é‡ - æ·»åŠ ç‰¹å¾å½’ä¸€åŒ–
        """
        with torch.no_grad():
            # é¢„å¤„ç†
            input_tensor = self.preprocess_image(image_path)
            
            # å‰å‘ä¼ æ’­
            features = self.model(input_tensor)
            
            # ç‰¹å¾å½’ä¸€åŒ– - ä¸StrongSORTä¸€è‡´
            features = torch.nn.functional.normalize(features, p=2, dim=1)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            features = features.cpu().numpy().squeeze()
            
        return features

    def extract_features_from_cv2(self, cv2_img):
        """
        ä»OpenCVå›¾åƒæå–ç‰¹å¾
        """
        with torch.no_grad():
            input_tensor = self.preprocess_cv2_image(cv2_img)
            features = self.model(input_tensor)
            features = torch.nn.functional.normalize(features, p=2, dim=1)
            features = features.cpu().numpy().squeeze()
        return features

    def extract_features_batch(self, image_paths):
        """
        æ‰¹é‡æå–ç‰¹å¾
        """
        features_list = []
        
        for img_path in image_paths:
            features = self.extract_features(img_path)
            features_list.append(features)
            
        return np.array(features_list)

    def calculate_similarity(self, features1, features2):
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ - ä½¿ç”¨å½’ä¸€åŒ–åçš„ç‰¹å¾
        """
        # ç¡®ä¿æ˜¯1Dæ•°ç»„
        features1 = features1.flatten()
        features2 = features2.flatten()
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = 1 - cosine(features1, features2)
        return similarity

    def compare_images(self, image_path1, image_path2, threshold=0.75):
        """
        æ¯”è¾ƒä¸¤å¼ å›¾åƒ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ” å›¾åƒæ¯”è¾ƒ: {os.path.basename(image_path1)} vs {os.path.basename(image_path2)}")
        print(f"{'='*60}")
        
        # æå–ç‰¹å¾
        start_time = time.time()
        feat1 = self.extract_features(image_path1)
        feat2 = self.extract_features(image_path2)
        extract_time = (time.time() - start_time) * 1000
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        start_time = time.time()
        similarity = self.calculate_similarity(feat1, feat2)
        similarity_time = (time.time() - start_time) * 1000
        
        print(f"ç‰¹å¾æå–è€—æ—¶: {extract_time:.2f}ms")
        print(f"ç›¸ä¼¼åº¦è®¡ç®—è€—æ—¶: {similarity_time:.2f}ms")
        print(f"ğŸ“Š ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.6f}")
        print(f"ğŸ’¯ åŒ¹é…åˆ†æ•°: {similarity*100:.2f}%")
        
        # åˆ†æç»“æœ
        predicted = 'same' if similarity > threshold else 'different'
        print(f"ğŸ”® é¢„æµ‹ç»“æœ: {predicted} (é˜ˆå€¼: {threshold})")
        
        self._analyze_similarity(similarity, threshold)
        
        return similarity, feat1, feat2, predicted

    def _analyze_similarity(self, similarity, threshold):
        """åˆ†æç›¸ä¼¼åº¦ç»“æœ"""
        if similarity > 0.9:
            print("âœ… æé«˜ç›¸ä¼¼åº¦ - æå¤§æ¦‚ç‡æ˜¯åŒä¸€ä¸ªäºº")
        elif similarity > threshold + 0.1:
            print("âœ… é«˜ç›¸ä¼¼åº¦ - å¾ˆå¯èƒ½æ˜¯åŒä¸€ä¸ªäºº")
        elif similarity > threshold:
            print("âš ï¸  ä¸­ç­‰ç›¸ä¼¼åº¦ - éœ€è¦è¿›ä¸€æ­¥éªŒè¯")
        elif similarity > threshold - 0.1:
            print("â“ ä½ç›¸ä¼¼åº¦ - å¯èƒ½ä¸æ˜¯åŒä¸€ä¸ªäºº")
        elif similarity > 0.3:
            print("âŒ æä½ç›¸ä¼¼åº¦ - å¾ˆå¯èƒ½ä¸æ˜¯åŒä¸€ä¸ªäºº")
        else:
            print("âŒ æä¸ç›¸ä¼¼ - ç¡®å®šä¸æ˜¯åŒä¸€ä¸ªäºº")

    def test_model_performance(self, test_cases, threshold=0.75):
        """
        æµ‹è¯•æ¨¡å‹æ€§èƒ½
        test_cases: [(img1, img2, expected_label), ...]
        """
        results = []
        
        for i, (img1, img2, expected) in enumerate(test_cases):
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•ç”¨ä¾‹ {i+1}: {expected}")
            print(f"{'='*60}")
            
            if not all(os.path.exists(img) for img in [img1, img2]):
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
                continue
            
            try:
                similarity, _, _, predicted = self.compare_images(img1, img2, threshold)
                
                correct = predicted == expected
                
                results.append({
                    'case': i+1,
                    'image1': os.path.basename(img1),
                    'image2': os.path.basename(img2),
                    'similarity': similarity,
                    'predicted': predicted,
                    'expected': expected,
                    'correct': correct,
                    'threshold': threshold
                })
                
                status = "âœ…" if correct else "âŒ"
                print(f"{status} é¢„æµ‹: {predicted}, é¢„æœŸ: {expected}")
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                results.append({'case': i+1, 'error': str(e)})
        
        # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        self._print_performance_report(results)
        return results

    def find_optimal_threshold(self, test_cases):
        """å¯»æ‰¾æœ€ä½³ç›¸ä¼¼åº¦é˜ˆå€¼"""
        thresholds = np.arange(0.5, 0.95, 0.05)
        best_accuracy = 0
        best_threshold = 0.75
        
        for threshold in thresholds:
            correct = 0
            total = 0
            
            for img1, img2, expected in test_cases:
                if not all(os.path.exists(img) for img in [img1, img2]):
                    continue
                
                feat1 = self.extract_features(img1)
                feat2 = self.extract_features(img2)
                similarity = self.calculate_similarity(feat1, feat2)
                predicted = 'same' if similarity > threshold else 'different'
                
                if predicted == expected:
                    correct += 1
                total += 1
            
            if total > 0:
                accuracy = correct / total
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_threshold = threshold
        
        print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}, å‡†ç¡®ç‡: {best_accuracy:.3f}")
        return best_threshold

    def _print_performance_report(self, results):
        """è¾“å‡ºæ€§èƒ½æŠ¥å‘Š"""
        if not results:
            return
            
        correct_count = sum(1 for r in results if 'correct' in r and r['correct'])
        total_count = sum(1 for r in results if 'correct' in r)
        
        accuracy = correct_count / total_count * 100 if total_count > 0 else 0
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {len(results)}")
        print(f"æ­£ç¡®è¯†åˆ«: {correct_count}/{total_count}")
        print(f"å‡†ç¡®ç‡: {accuracy:.1f}%")
        print(f"ä½¿ç”¨é˜ˆå€¼: {results[0]['threshold'] if results else 'N/A'}")
        
        # æ˜¾ç¤ºé”™è¯¯æ¡ˆä¾‹
        errors = [r for r in results if 'correct' in r and not r['correct']]
        if errors:
            print(f"\nâŒ é”™è¯¯æ¡ˆä¾‹:")
            for error in errors:
                print(f"  ç”¨ä¾‹{error['case']}: {error['image1']} vs {error['image2']}")
                print(f"     ç›¸ä¼¼åº¦: {error['similarity']:.4f}, é¢„æµ‹: {error['predicted']}, é¢„æœŸ: {error['expected']}")

def main():
    # æ¨¡å‹è·¯å¾„
    model_path = "./osnet_x0_25.pth"
    
    # æµ‹è¯•å›¾åƒç›®å½•
    test_dir = "./test_image/"
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        (os.path.join(test_dir, "0001_c5_f0051487.jpg"), 
         os.path.join(test_dir, "0001_c5_f0051607.jpg"), "same"),
        
        (os.path.join(test_dir, "0001_c5_f0051487.jpg"), 
         os.path.join(test_dir, "0007_c2_f0047184.jpg"), "different"),
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ {model_path}")
        return
    
    # åˆ›å»ºæ¨ç†å™¨
    print("åˆå§‹åŒ–PyTorchæ¨ç†å¼•æ“...")
    inference_engine = OSNetPyTorchInference(model_path)
    
    # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
    print("\nå¯»æ‰¾æœ€ä½³é˜ˆå€¼...")
    optimal_threshold = inference_engine.find_optimal_threshold(test_cases)
    
    # ä½¿ç”¨æœ€ä½³é˜ˆå€¼æµ‹è¯•æ¨¡å‹æ€§èƒ½
    print(f"\nä½¿ç”¨æœ€ä½³é˜ˆå€¼ {optimal_threshold:.3f} æµ‹è¯•æ€§èƒ½...")
    results = inference_engine.test_model_performance(test_cases, optimal_threshold)
    
    # ç‰¹å¾åˆ†æï¼ˆå¯é€‰ï¼‰
    if input("\næ˜¯å¦è¿›è¡Œç‰¹å¾åˆ†æï¼Ÿ(y/n): ").lower() == 'y':
        image_paths = []
        labels = []
        
        for img1, img2, expected in test_cases:
            image_paths.extend([img1, img2])
            label1 = os.path.basename(img1).split('_')[0]
            label2 = os.path.basename(img2).split('_')[0]
            labels.extend([label1, label2])
        
        # æå–ç‰¹å¾å¹¶åˆ†æ
        features = inference_engine.extract_features_batch(image_paths)
        print(f"\nç‰¹å¾å½¢çŠ¶: {features.shape}")
        print(f"ç‰¹å¾èŒƒå›´: [{features.min():.6f}, {features.max():.6f}]")
        print(f"ç‰¹å¾å‡å€¼: {features.mean():.6f}")
        print(f"ç‰¹å¾æ ‡å‡†å·®: {features.std():.6f}")

if __name__ == "__main__":
    main()